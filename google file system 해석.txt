abstruc

데이터 집약적인 앱을 위해 엄청나게 많은 데이터가 쌓여가는 이 시점에서 GFS의 확장가능한 버전을 다시 디자인해야했다. 
그 디자인의목표는 하드웨어중 일부가 고장이 나더라도 정상적인 기능을 제공하고, 많은 클라이언트에 질 좋은 정보를 제공해야한다.

앞선 DFS는 같은 목적을 공유하는데, 이는 과거에 좋은 타겟이고 현재는 다양한 관점의 작업들이 많아지면서 같은 목적을 공유해서는
한계가 있고 전통적인 디자인을 검토하고 다시 다른 디자인을 생각해본다

이ㅣ 파일시스템은 우리 저장공간에 대한 요구사항을 만족하게 됬고,  대규모 데이터셋이 요구되는 연구개발활동에 모든 분야에
구글 내에 광범위하게 배포됨. 지금까지 클러스터들은 수천대의 머신에서 수천개의 디스크를 통해 수백테라를 제공하고
동시에 수백대의 클라이언트들이 접근한다.

이 논문은 DFS을 위해 설계한 인터페이스 범위, 설계의 대한 설명, 벤치마킹, 실제 수치를 보여준다.

-------

intsroduction

perfomance, scalabtility, reliability, availability
component failure / traditional standards / mutated data / simplify file system

구글의 빠른 데이터 프로세싱 요구를 충족하기 위해 GFS 설계 및 구현,
GFS는 성능, 확장성, 신뢰성, 유효성과 같이 기존의 파일시스템과 많은 목표들을 공유한다.
그러나 이전 파일 시스템 설계 과정과는 다른 현재 예상되는 응용프로그램의 작업 부하 및 기술적 환경에 대한 주요 관측에 의해 설계
우리는 기존의 선택을 재검토하고 다지인 공간에서 다른점을 탐구함

첫번째로 기존과 다른 점은 FS의 구성요소의 오류는 일반적이라는 것이다. 파일 시스템은 수백대에서 수천대의 스토리지 장비에
비슷한 클라이언트 장비에 의해 접근되는데. 구성요소가 많아질수록 그 중 하나는 제대로 작동하지 않거나 복구되지 않는 것들이
필연적으로 생긴다. 이 원인을 어플의 버그,os버그, 사람, 디스크,메모리,커넥터,네트워크,파워의 고장인 것을 보아왓다
그래서 끊임없는 모니터링, 에러검출, 내부 결함, 자동복구는 완벽해져야 한다.

둘째로 전통적인 표준의 거대한 파일은 수십GB정도가 보통이고 그것들은 웹문서나 응용프로그램의 객체를 담는다. 그러나
수십억 객체로 이루어진 TB범위로 커지는 데이터를 다룰때는 파일시스템이 수십억개의 kb파일을 관리하기가 어렵다.
결과적으로 io작업 및 블록 크기등의 한도와 설계를 재검토해야했다.

셋째, 대부분의 파일은 데이터를 덮어씌우는 것보다 데이터가 추가 됨에따라 변형되는 경우가 대부분이다. 한번파일이 쓰이면 ㅇ
그파일은 읽히기만하거나 순차적으로 이어붙여진다. //데이터의 이런 특성은 분석프로그램을 통해 대형저장소를 구성가능 // 
또한 실행중인 앱에 의해 지속적으로 데이터스트림이 생성됨. 일부는 데이터를 보관가능. 일부든 동시든 늦게든
중간의결과를 하나의 장비에서 생산하고 다른 곳에서 처리될수 있다. 거대한 파일에대한 이런 접근은 데이터블록을 클라이언트에서
캐싱하는 부분에서  아쉽지만 성능 최적화, 원자성의 보증이되고 초점이된다..

넷째, 공동설계 앱과 파일시스템의 API는 전체 시스템의 이익이 된다. 예를들면 앱에 대한 부담스런 규제가 없어도 크고
단순하게GFS의 일관적인 모델을 완화했다. 또한 클라이언트 사이간에 추가적인 동기화 과정 없이 하나의 파일을 추가할수있도록
원자성을 연산에 추가해 도입-이건 논문 뒤에서 자세하게 설명 //

여러 GFS은 서로 다른 목적을 위해 현재 배치되있고. 최근 하나는 1000노드 초과 300TB의 디스크 저장소를 초과.
개별적인 장비는 수백 클라이언트에 의해 많이 접근되고있음.

2. DESIGN OVERVIEW

2.1 assumptions -가설
 GFS의 주요한 관점에 대해 좀더 세밀하게 가설을 세운다.

 1. 시스템은 고장 날 수 있는 값싼 많은 상품 요소로 지어졌다. 시스템은 일상적인 구성 요소 실패에 대해 지속적으로 자신을 모니터링
해야하고 탐색해서 내성을 갖고 신속하게 복구해야한다.

 2. 시스템은 적당한 수의 큰 파일을 저장한다. 일반적으로 100mb정도 아니면 좀더 큰 수백만개의 파일을 예상. 
수 Gb파일은 일반적이며 그것들을 효율적으로 관리. 또 작은 파일들을 지원해야한다.그러나 그것에 최적화 할 필요 없음

 3. 부하는 주로 대용량 스트리밍 읽기와 임의의 작은 파일을 읽는 것이다. 대용량 스트리밍에서 한번에 수백MB를 읽거나 일반저긍로
1MB이상 읽는다. //동일한클라이언트에서 파일은 인접영역을 통해 읽는다.// 임의의 작은 파일은 임의의 오프셋에서 몇개의 kb를
읽는데 이는 성능에 민감한 앱에서 파일을 앞뒤로 이동하지 않고 꾸준히 처리하기위해 작은 읽기작업을 모아서 정렬후 일괄적으로 읽는

 4. 부하는 또한 파일들에 데이터를 추가하려는 매우크고 순차적인쓰기. 일반적인 동작의 크기는 읽기위한 것과 비슷함.
한번 쓰여진 파일은 거의 수정되지 않는다. 하나의 파일은 임의 위치에 작게쓰는건 지원되지만능률적이기 위해선
그것은 하지 않아야함.

 5. 시스템은 같은 파일에 대해 여러클라이언트들에게 잘 배부되도록 구현해야한다. 주로 파일은 시스템간?에 --// 하나의 파일에동시에접근할때)
큐나 많은 방법으로 병합된다. 컴퓨터당 실행되는 수백개의 머신은 동시에 파일을 추가한다. 최소한의 동기화 오버헤드를 갖는 원자성
은 필수이다. 파일을 나중에 읽을수도있고 , 동시에 컨슈머가 읽을수도있다.

 6. 결합된 높은 대역폭은 딜레이보다 훨씬 더 중요.우리의 목표 대부분은 개별적인 읽기나 쓰기에 대해 엄격한 응답 요구사항을
가지고 있는 동안 어플리케이션이 데이터를 처리하는데 높은 비율로 데이터를 프로세싱하는 것을 중요하게 여김

2.2 interface 
 GFS는 파일이 디렉토리 계층과 경로명으로 식별되어 조직화시키고, 파일에 대해 생성, 삭제, 열기 , 닫기, 읽기, 쓰기와 같은
일반적인 연산을 제공한다. 또 스냅샷과 레코드 추가 연산도 제공한다.
스냅샷은 저비용으로 파일 또는 트리 구조의 디렉터리 복사본 생성
레도드 추가는 각 개별적인 클라이언트 추가 연산에 대해 원자성을 보장하는 동안 여러 클라이언트들에게 동시에 같은 파일에 대해
데이터를 추가하는 것을 허용. 이건 많은 클라이언트ㅡㄹ이 락 없이 동시에 추가할 수 있는 다중 머지결과들과 프로듀서-컨슈머를
구현하는데 유용. 

2.3 
 GFS 클러스터는 하나의 마스터와 여러개의 청크서버들로 구성되며 여러 클라이언트들에 접근되어진다. 
청크서버는 리눅스 서버프로세스로 돌아가는 리눅스 머신이다.서버의 자원을 허용하고 이상한 앱의 코드를 받아들여 신뢰성이
낮아지는 한 같은 머신에서는 대규모 서버와 클라이언트를 구성하기 쉽다.

파일은 고정된 사이즈의 조각들로 나눠짐. 각각의 조각(청크)는 조각 생성시점 마스터에의해 64비트의 조각번호를 부여받고 청크서버에들에
로컬디스크에 저장하고 신뢰를위해 다수의 정크서버에복사. 사용자가 네임스페이스의 다른 영역에 서로 다른 복제 수준을 지정할수
잇고, 기본적으로 세개의 복사본을 저장.

마스터는 파일의 메타데이터 구성, 네임스페이스, 파일조각매핑, 조각의 위치. 조각의 대여관리 분실된 조각에 대해
GC 정크서버들간의 조각이동과 같은 전반에 걸친 활동 제어. 마스터는 지시를 전달하고 상태 수집하기 위해
정크서버와 하트비트 메세지를 주기적으로 통신

GFS 클라이언트 코드는 이 파일시스템 API를 적용하고 있는 각각의 어플리케이션과 링크되어 어플리케이션전반에 걸친 마스터와 
조각서버간의 통신에 관여하게 된다.  클라이언트는 마스터와 통신하며 메타데이터 관련 작업을 수행하지만 실제 데이터 관련 통신은 
모두 조각서버로 직접 전달된다.  이시스템은 POSIX레벨의 API를 제공하지 않기 때문에 리눅스 v노드계층을 후크할 필요가 없다.

클라이언트나 조각서버 어느쪽도 파일 데이터를 캐쉬하지 않는다.  대부분의 어플리케이션은 거대한 파일의 스트림을 읽어들이거나 
캐쉬하기에 너무 큰 사이즈의 데이터를 다뤄야 하기 때문에 클라이언트의 캐쉬는 거의 유용성이 없으므로, 클라이언트에서 캐슁 기능을 
제거해서 시스템 전반의 동조화에 관련된 문제점을 해결할 수 있다. (메타데이터는 클라이언트 캐쉬에 저장딘다)  각각의 조각 파일은 
로컬 파일로 저장되므로 리눅스의 버퍼캐쉬가 지속적으로 메모리에 데이터를 저장하기때문에 조각서버 역시 파일 데이터를 캐쉬에 
저장할 필요가 없다.

2.4 Single Master
마스터를 가짐으로써 설계를 단순화하고 전체적인 관리를 할수있게 한다. 그러나 병목현상이 발생할수잇다.
 master는 메타데이터만 클라이언트에게 제공하고 파일 제공은 하지 않는다. 이로서 병목현상을 해소한다.

가장 단순한 형태의 읽기 작업을 그림1과 함께 설명해보면, 우선 클라이언트는 지정된 조각 사이즈에 따라 어플리케이션이 배정한 
파일의 이름을 분석하고 파일내의 조각 인덱스 오프셋을 계산한다. 그러고나서 마스터에게 파일이름과 조각인덱스를 포함한 요청을 
전송하면 마스터는 해당 조각핸들과 복제의 위치를 반환하게되는데, 클라이언트는 이정보를 파일이름과 조각인덱스를 키로 삼아 
캐쉬에 저장한다.  클라이언트는 이후 복제중 하나 - 대부분 가장 가까운 복제 - 에 요청을 보낸다.  이 요청은 조각 핸들과 그 조각 
내의 구체적인 바이트 범위를 포함하는데, 동일한 조각에 대한 읽기 작업은 캐쉬에 저장된 이 정보가 만료가 되거나 파일이 다시 열리기
 (reopened) 전에는 마스터와의 통신이 필요없게 된다.
 
2.5 Chunk Size
 
일반적인 파일시스템보다 큰 64mb를 사용함으로써 
 (lazy space allocation)을 사용하여 대형 사이즈의 조각(64MB)을 사용함으로써 
야기될수 있는 가장 큰 문제중 하나인 내부 파일 조각화에 의한 공간 낭비를 줄였다.
장점; 클라이언트가 조각 정보에 관해서 마스터와 통신해야 하는 횟수를 줄여준다. 일반적으로 애플리케이션은 대용량 파일을 읽고쓰기때문에
 클라이언트가 해당 조각 내에서 더 많은 작업을 수행할 수 있다. 고정된 TCP연결을 유지해야할 필요가 없어 네트웍의 부하를 줄일 수 있다.
 전체 메타데이터의 크기를 작게 만들어 준다. // 작은청크정보는 청크사이즈가 커질수록 작은청크 로케이션 -- *

단점: 작은파일 이 파일을 동시에 여러 클라이언트가 접근할 경우 이러한 조각을 저장하는 조각서버는 분쟁지역(hot spot)화 할 수 있다.
사실 큰 문제는 아니였다 순차적으로 청크 파일을 읽었기 때문인데
 분쟁 지역 발생 문제는 GFS가 초기 일괄 처리 시스템 (batch-queue system)에 적용되었을때 발견되었는데, 과부하가 심해서
 더 높은 복제율로 저장하도록 하고 각각의 일괄 처리 시스템들은 어플리케이션의 실행 시점을 미묘하게 다르게 갖도록 했다.  
좀 더 근본적인 해결책으로는 클라이언트가 이런 분쟁지역이 발생할 경우 다른 클라이언트로부터 데이터를 읽어들이도록 하게 하는 것
 
2.6 Metadata
마스터에는 파일과 조각의 네임스페이스, 파일과 조각간의 매핑 정보, 그리고 각 조각의 복제들의 위치정보, 
크게 이 세가지의 메타데이터가 저장된다.  세가지중 앞 2개는 마스터 로컬디스크에
작업로그로 영구보존된다.또한 리모트머신에 백업도한다.
 로그를 사용하면 간단하고 안정적으로 마스터 상태를 업데이트 할수 있고 일관성 유지가능
이는 마스터의 메모리에 보관된다.
마스터는 청크위지정보를 지속적으로 저장하지 않는다. 대신 마스터 시작시 청크서버가 클러스터에 참여할 때마다 청크서버에게
해당 청크에 대해 묻는다.
 
2.6.1 In-Memory Data Structures
최악의 경우 거대한 파일 시스템을 구성하기위해 메모리가 부족한 경우라 할지라도, 마스터에 추가로 메모리를 확장하는 것은 
모든 메타데이터를 메모리에 저장함으로써 얻을 수 있는 간편함과 안정성, 속도, 가변성을 생각하면 저렴한 비용이라고 간주할 
수 있을 것이다.
이러한 설계가 수백개이상의서버로구성된클러스터에서는흔하게발생할수있는 문제들 - 조각서버들이클러스터에추가되거나삭제되고,
이름을바꾸거나오류발생,재기동등의문제 - 속에서도 마스터와조각서버간의동기를맞추는작업을간편하게할수있다.
각 청크는 64mb데이터에 64바이트미만의 메타데이터가 생긴다.  

2.6.2 chunk locations
마스터는 어떤 청크서버가 주어진 청크의 replica를 가지고 있는지 지속적인 기록을 유지하지 않는다. 시작시 해당 정보에
간단히 서버를 폴링한다. 마스터는 모든 청크 배치를 제어하고 정기적인 하트비트로 청크 서버상태를 모니터링 해서
이후 자신을 최신상태로 유지할수 있다. 원래는 마스터에서 청크위치를 지속적으로 유지할려 했지만 시작이후 정기적으로 받아오는게
더 간단하다고 생각함. 왜냐면 청크서버가 클러스터에 참가 , 탈퇴, 변경, 실패, 리붓의 이유로 마스터와 청크서버를 동기화 상태로
유지하는 문제가 없어짐. 수백개의 서버가잇는 클러스터는 이러한 이벤트가 너무자주발생. 또 다른 이유는 청크서버가 자신의 디스크에서
수행할 것인지 가지지 않을것인지 결정하는 것. 청크서버의 오류로 청크가 자발적으로 사라지거나 운영자가 청크서버이름바꿀수있어서
마스터에서 이정보의 동기화를 유지할 필요가 없음.

2.6.3 Operation Log
 작업 로그는 메타데이터에 대해 치명적일 수 있는 변경의 순차적 기록을 저장한다. GFS의 핵심이다.
메타데이터는 유일한 지속적인 기록일 뿐만 아니라 동시 작업 순서를 정의하는 논리적인 시간선으로도 사용.
파일과 조각들은 버전을 포함해서 (4.5항 참조) 모두 생성된 논리적 시간대에 의해서 단일하고 영구적으로 구분된다.
이게 오류나면 전체적인 파일에 문제가 생긴다.
작업로그는 원격 기기에 다시 복제되어 해당하는 로그 기록을 로컬과 원격 모두의 디스크에 갱신을 마친후에 클라이언트 작업에 
응답하도록 한다.
마스터는 이 작업 로그를 다시 수행함으로써 파일시스템의 상태를 복구한다.  재기동 시간을 줄이기 위해서는 로그의 크기가 작도록 
유지해야하는데, 마스터는 로그가 특정 크기를 넘어설때마다 기록을 해두고 (checkpoint) 복구시에는 이것을 로컬 디스크에서
 읽어들여서 다시 수행해야 하는 로그 기록의 갯수를 최소화 한다.  이 기록은 메모리에 직접 저장할 수 있는 압축된 B트리 형태로
 저장되어 추가적인 분석과정 없이도 네임스페이스 검색에 이용될 수 있다.
이런로그는 복구속도가 빨라지고 가용성이 향상된다.
검사점 작성은 시간이 오래걸릴수잇어서 지연시키지 않고 계속 새로운검사점을 만들도록한다. 검사점직으면 새로그
파일로 전환하고 별도의 스레드의 새검사점을 만든다.  새로운 체크포인트는 스위칭 전의 모든 변화가 포함된다.
수백만개파일이있는 클러스터의경우 1분정도 걸린다. 완료되면 로컬및원격디스크에 기록
복구에는 최신 검사점 이후 로그만 필요하고 그전 파일은 자유롭게 삭제할수도잇대/
기록을 작성하는 동안에 발생한 오류는 복구코드가 자동으로 검색하여 불완전하게 종료한 기록들을 무시하기 때문에 전체 데이터의 
무결성에 영향을 미치지 않는다.

2.7 
 GFS의 보증과 의미에 대해 설명한다.

2.7.1
파일 네임스페이스의 변경은 - 예를들어 파일의 생성과 같은 - 각각의 원자성을 가진다.  이것들은 마스터에의해서만 처리될수 있다: 네임스페이스의 잠금 기능이 원자성과 정확성을 보장한다
 데이터 갱신 이후의 파일 영역 상태는 그 갱신 작업의 종류와 갱신 성공 여부, 또 병렬로 진행된 또 다른 갱신작업의 유무에 따라 달라진다.  여기서 가능한 모든 경우를 테이블 1에 요약해 보았다. 
region은 데이터 수정이 된 이후 정의 되며 클라이언트는 수정이 어떻게 되는지 확인한다.
consistent : 다 똑같은걸 보고잇는것
defined : 동시 작성자의 간섭 없이 수정이 성공 한 경우 모든 사람은 수정된걸 확인 할 수 있음
undefined inconsistent : 동시에 성공한 수정. 모든 클라이언트는 동일한 데이터를 보지만 하나는 반영하지 않을 수 있다.
일반적으로 여러 수정된거의 모음으로 구성됨
inconsistent(undefined) : 수정이 실패한 경우 클라이언트가 서로 다른시간에 서로 다른 데이터를 볼 수 있다.

이 아래에서는 응용프로그램이 undefined공간을 더이상 구분하지 않아도 되는 이유를 설명한다.

데이터 변이는 쓰기 또는 추가일 수 있다. 쓰기로 인해 응용 프로그램에 지정한 파일 위치에 데이터가 기록된다. 레코드 추가는 데이터를
동시에 추가할 수 있지만(병렬) GFS가 선택한 파일위치에서는 데이터가 최소 한번 추가하는것.일반적인건 파일의 끝에 이어쓰는것
 파일오프셋가 클라이언트로 반환되고 레코드가 포함된 defined 영역 시작을 표시한다. 

또한 GFS는 의미없는단어를넣어서 고정길이를 만들거나 중복을 기록할 수 있다. 이는 일관성이 없는 것으로 간주되는 영역을 가지며
사용자 데이터에 양에 비해 작다 031 201 3265 9--.

수정이 성공적으로 된다면, 수정파일 영역이 정의되고 마지막 수정에 의해 기록된 데이터가 포함된다. 
1.gFs는 모든 복제본에 동일한 순서로 청크에 수정사항을 적용하고
2.청크 손실로 인해 오래된 복제본을 탐지하기 위해 청크버전번호를 사용해 청크서버가 다운된 동안 수정안된 오래된 것을 찾아낸다.

복제본은 절대 수정에 연루되거나 마스터에게 청크 위치를 묻는 클라이언트에게 제공하지 않는다. 
그들은 최대한 오래된 청크를 수거한다. 클라이언트는 청크위치를 캐시하므로 오래된 복제본에서 읽어올수 있으므로 다음에 그 파일을 오픈하려고하면
이 청크의 모든 캐시를 삭제한다. 오래된복제본은 아예 쓰래기값을 가져오게 만들어버린다.대게 오래된청크는 끝부분이 어팬드라 좀이상해서
그리고 마스터에게 재접촉하면 위치 갱신된 청크 위치를 가져온다.

수정 성공후 디스크의 고장, 손실이 일어날 수 있으므로 GFS는 마스터 서버와  모든 청크 서버간의 정기적인 하트비트로
손상을 감지한다. 문제가 되면 유효한 복제본에서 복원되고. 일반적으로 모든복제본이 손실된경우 되돌릴수없게 손실시킨다.
이경우에는 앱이 손상된데이터가아니라 명백한 오류를 뜨게한다.

2.7.2 앱에 대한 영향
 GFS은 간단한 기술로 완화된 모델을 수용 할 수 있다. 덮어쓰기, 자기점검, 하트비트 체크포인트 레코드보단 추가에 의존한다...?

실제로 모든 앱은 덮어쓰기가 아니라 추가함으로 파일을 변형한다. 한가지 일반적인 상황에서 작가는 파일을 첨부터 끝까지 생성한다.
모든 데이터를 생성 후 파일이름을 영구적으로 지정하거나, 얼마나 성공적으로 작성했는지 주기적으로 체크포인트로 변경한다.

체크포인트에는 앱 수준 체크섬도 포함된다. readers는 정의된 상태로  마지막 체크포인트까지 파일 영역만 확인하고 처리한다.
일관성과 동시성 문제 없이 큰 도움이 된다. 추가는 랜덤 라이트보다 훨씬 효율적이고 장애에대한 복원력이 높다.

체크 포인팅은 작가가 단계적으로 다시 시작하는 것을 허용하고 
응용 프로그램의 관점에서 아직 불완전한 파일 데이터를 제대로 처리하지 못하도록 할수 잇다.
ex데이터가 1~10가 다 필요한 상황에서 5에서 찍혀잇으면. 브레이킹포인트??

다른 일반적인 경우는 많은 작성자가 있어 병합된결과 또는 생산-소비자 큐로 동시에 파일에 추가한다. 레코드 추가 1회 이상은
작성자의 출력을 보존한다.

리더s는 패딩과 중복을 다음과 같이 처리한다. 쓴사람의 레코드에는 체크섬과 같은 추가정보가 있어 유효성을 확인 할 수 있다.
리더s는 체크섬을 사용하여 추가 패딩을 식별하고 폐기하고 조각들을 기록한다. 때때로 중복되는 항목을 허용할 수 없는 경우
기록에서 고유한 식별자를 사용해 해당 항목을 필터링, 이 식별자는 웹가 같은 해당 앱의 엔티티 이름에 항상 필요

기록 i/o는 google에서 제공하는 라이브러리 코드며 구글의 다른 인터페이스 구현에 적용된다. 이와함께 동일한 기록과 레코드판독기에
전달된다.

3
 마스터가 운영에 관여하는 것을 최소하 하기 위해 시스템 설계. 그래서 클라이언트, 마스터, 청크서버, 데이터 수정, 레코드 어펜드, 스냅샷 설명

3.1
수정은 쓰기, 추가, 메타데이터 변경 작업이다. 각 수정은 청크의 모든 복제본에서 수행된다. 여러 복제본에서 일관된 수정본을 유지하기 위해
리스 개념을 사용한다. 마스터는 모든 복제본 중 한개에 청크 리스를 부여한다. 이걸 primary라고 한다.
primary는 청크에 대한 모든 수정의 일련 순서를 선택한다. 모든 복제본은 수정을 할때 이 순서를 따른다.
따라서 전체 수정순서는 마스터가 선택한 리스 허가 순서와 primary가 할당한 수정 순서로 정의된다.
리스 매커니즘은 마스터의 관리 오버헤드를 최소화하도록 한다. 리스의 초기 timeout은 60초다.
그러나큰 청크가 수정요청은 마스터로부터 무한정 확장 요청 가능하며 이런 확장 및 요청은 하트비트 메세지에
돼지꼬리처럼 따라다닌다. 마스터는 리스가 만료되기 전에 리스가 해지되는 경우가 있다. 변경중인 파일에서
변이를 중지하려는 경우. 마스터는 primary와 통신이 끊겨도 리스 기간이 만려된후 다른 복제본에게
다시 임대계약을 안전하게 부여한다.
과정은
1. 클라이언트는 primary가 누군지 마스터에게 묻는다. primary가 없는 경우 마스터는 primary를 부여
2. 마스터는 프라이머리랑 나머지 복제본 위치로 응답. 클라이언트는 향후 변이를 위해 이데이터를 캐시한다.
프라이머리에 연결할수없거나 더이상 리스를보유하지 않는경우에만 마스터에 다시연락.
3. 클라이언트는 모든 복데본에 데이터 푸시. 각 청크서버는 데이터가 사용되거나 오래될때까지
LRU버퍼케시에 저장. 제어흐름에서 데이터를 분리함으로써 청크서버가 일차적잉ㄴ관계없이
네트워크에 기반한값비싼 데이터흐름 스케쥴링이 가능하고 성능개선.
4. 모든 복제본이 데이터 수신을 승인하면 클리아인트는 프라이머리한테 쓰기요청보냄. 이요청으로 모든 복제본에
이전에 푸시된데이터 캐시에서 가져온다. 기본 일련번호는 다른클라이언트에서도 수정요청을 보낼수 있으니
알아서 잘 일련번호를 만든다. 그것을 변히 일련번호 순으로 자신의 상태에 적용한다.
5. 프라이머리는 쓰기요청을 다른 복제본에도 다 적용 이건 일련번호 똑같이주는대로 쭉간다.이게 아 수정요청
6. 프라이머리에게 복제완료됫다고 말함
7. 클라이언트에게 응답한다 오류라도 클라이언트에게 모두 보고 안됫으면 인련번호할당안됫다구하고
요청 실패한것으로되서 수정영역이 일관되지 않은 것으로 유지된다.
그래서 클라이언트 코드는 실패한 수정을 재시도하여 이런 오류를 처리한다. 3~7을 몇번시도한다.그래도안되면쓰기부터다시
응용프로그램에의한 쓰기가 커서 청크 경계에 걸리면 GFS는 이를 여러개의 쓰기작업으로 나누고
모두 제어흐름 따르지만 다른클라이언트 동시작업에 인터리빙되고 덮어쓸수있다. 그래서 개별작업이
모든 복제본에서 동일순서로 완료되서 복제본은 동일하지만 컨시스턴트하지만 언디파인된다.

3.2
네트워크 효율적으로 사용하기 위해 데이터 흐름 제어를 흐름에서 분리한다. 제어가 클라이언트에서
프라이머리에게로 그리고 모든 보조로 흐르는 데이터는 청크서버 체인따라 파이프라인 방식으로 선형적으로
푸시된다. 목표는 네트워크 대역폭을 최대로 활용하고, 네트워크 병목같은 지연시간 방지, 데이터 처리하기
위한 대기시간 최소화.

각 시스템의 대역폭을 최대한 활용하기 위해 데이터는 다른 토폴로지(트리)에 분산되지 않고 청크서버에 체인을
따라 선형적으로 푸시된다. 따라서 각 시스템의 전체 아웃바운드 대역폭은 여러 수신자로 분할되지 않고
최대한 빨리 전송하는데 사용한다.

네트워크 병목현상같은건 네트워크 토폴로지의 가까운데보내는것으로으로 처리한다. 데이터를 가까운
청크서버로 바로쏴서 징검다리같은걸로 이건 IP주소에서 거리를 정확히 추정할만큼 간단하다.

마지막으로 TCP연결로 데이터 전송을 파이프라인을 통해 지연시간을 최소화한다. 
데이터를 받으면 바로 발송준비한다. 이건 파이프라인이 아주 유용하다.파이프라인은 스위치네트워크
를해서 데이터를 즉시전송해도 수신속도가 줄어들지 않는다. 
네트워크 정체가 없으면 B바이트를 R복제본으로 전송하는데 이상적인 결과 시간은 B/T + RL이다.
여기서 T는 네트워크 처리량, L은 두시스템간의 전송대기시간 일반적으로 100mbps(T)며 L은
1ms미만 따라서 1mb는 80ms내에 이상적으로 배포 가능

3.3 atomic record Appends
구글파일시스템은 레코드 추가를 지원한다. 기존의 쓰기에서는 데이터를 기록할 위치를 지정한다.
동일영역에 대한 동시쓰기는 직렬화할수 없지. 이 영역에서는 여러 클라이언트의 데이터조각이 포함돼
이걸 어떻게 해결하냐 . 뭐 또한 클라이언트는 레코드 추가하는 데이터만 지정한다.(릴렉시브모델)
 뭐 별다른 게 없는데 어캐 이걸 GFS에서는 얘가 선택한 위치에서 최소 한번 파일을 첨부하고 해당 오프셋을
클라이언트에게 반환하고. 이건 여러 라이터가 동시에 조건없이 유닉스의 어펜드 모드에서 열린파일 쓰는 것과
유사하다. 레코드 추가는 여러 클라이언트가 동시에 동일파일에 추가하는 분산앱에서 많이 사용됨
기존의 쓰기에서는 잠금을 거는등 값비싼 동기화가 필요할것이다. 일에서 이런 파일은 여러 생상자
단일 소비자가 대기행렬을 하거나 다른 클라이언트의 병합된 결과를 포함하는 경우가 많다.

레코드 추가는 수정의 일종이며, 3.1의 제어흐름을 따른다. 클라이언트는 모든 레플리카에게 수정요청데이터를
보내고 프라이머리에게 요청을 한다.
프라이머리는 현재 레코드를 청크에 추가하면 64mb가넘는지 우선 확인한다. 넘는 경우 청크를 64mb로 패딩하고
나머지 레플리카들에게 똑같은 작업을 명령하고 청크들에게 동일한 작업을 다시시도해야한다는걸 클라이언트에
응답을 한다.(레코드 추가는 허용가능한수준이 64mb 1/4만큼만)

일반적인경우 (청크크기 안넘는경우) 프라이머리에서는 데이터를 복제본에 추가하고나머지 레플리카들에게도
정확한 위치에 데이터 작성하도록 하고 최종적으로 클라이언트에게 성공을 회신함.

복제본에서 레코드 추가가 실패하면 클라이언트가 작업을 재시도한다. 따라서 동일한 청크의복제본은 동일한 
레코드의 전체 또는 일부를 복제하는 등 서로 다른 데이터를 포함할 수 있음. gfs는 모든 복제본이 바이트 단위임을
보장하지 않는다. 단지 데이터가 적어도 한번 원자력 단위로 쓰여지는걸 보증한다.
이 속성은 작업이 성공을 보고하려면 일부 청크의 모든 복제본에 대해 동일한 오프셋으로 데이터를 기록해야한다는
기록으로 알 수 있다. 또한 이후 모든 복제본은 최소한 레코드 끝의 길이이므로 나중에 다른 복제본이
프라이머리가 될때  좀더 높은 오프셋 또는 다른 청크가 할당이된다.
일관성 보장 측면에서, 기록추가작업에 성공한 데이터가 기록된 영역이 정의되는 반면, 방해영역은 2.7.2처럼
일관되지않을수잇다

3.4 snopshot
스냅샷은 진행중인 수정의 중단을 최소호화하면서 파일 또는 디렉터리트리의 복사본을 거의 즉각적으로 만든다.
사용자는 이 툴로 대규모 데이터 세트의 복사본을 신속하게 생성하거나, 나중에 쉽게 커밋하거나 롤백 할 수 있는
변경사항을 테스트 하기 전 현재상태를 체크포인트 한다.
우린 표준쓰기 복사기술로 스냅샷 구현. 마스터는 스냅샷 요청을 받으면 먼저 스냅하려는 파일의 청크에 있는
리스를 취소한다. 이후 이러한 청크에 쓸경우 리스를 찾기위해 마스터와 상호작용해야한다.
이렇게 해야 마스터가 먼저 청크의 새 사본을 바로 만들수있다.

프라이머리가 취소되거나 만료된 후 마스터는 작업을 디스크에 기록한다. 그 뒤 카피할 데이터를 복제해 이 로그
레코드를 메모리 내 올린다.
새로 생성된 스냅샷 파일은 원본 파일과 동일한 청크를 가르킨다.
스냅샷 후 클라이언트가 처음으로 청크C에 기록하려고 할때 마스터에게 현재 프라이머리가 누군지 찾는 요청보냄
마스터는 청크C가 프라이머리가 2갠걸 알아차버린다. 클라이언트 요청에 거부하고 대신 새 청크 핸들C를 선택
그 뒤 C의 현재 복제본이 있는 각 청크 서버에 c'라는 새 청크를 만들것을 요청
원본과 동일한 청크서버가 새청크를 만듬으로 네트워크보다 로컬에서하니 훨씬빠르게 생성 보장한다. 
그리고 이후 똑같음 마스터는 복제본에서 리스부여하고 클라이언트에게 회신. 이 응답은 기존청크에서 
방금만들어졌음을 모를정도록빠르게 정상적으로 청크 작성 한다.

4. MASTER OPERATION
마스터는 모든 네임스페이스 작업을 실행합니다. 또한 시스템 전체에서 청크 복제본을 관리합니다. 위치를 어디로할지
결정을 내리고, 새로운 청크 및 복제품을 생성하고, 청크를 완전히 복제하고, 청크서버의 부하 균형을 맞추고,
사용되지 않는 스토리지를 회수하기 등 시스템 전체활동 조정.

4.1 네임스페이스 관리 및 잠금
예를 들어 스냅샷 작업은 스냅샷으로 덮인 모든 청크의 청크 서버리스를 해지해야하는등 마스터 작업에 오래걸릴 수 있다.
실행 중인 작업을 다중화로 작업하고 네임스페이스 영역에 대한 잠금을 통해 올바른 직렬화를 시켜야한다.
대부분의 기존 파일 시스템과 달리 GFS는 해당 디렉토리의 모든 파일을 나열하는 디렉토리별 데이터 구조가 없다.
또한 동일 파일이나 디렉토리에 대한 별칭도 지원하지 않는다. 
GFS는 전체경로 이름을 메타데이터에 매핑하는 룩업 테이블로 네입스페이스를 논리적으로 표현
접두사 압축을 표현하면 메모리에 효율적으로 표시 할 수 있다. 네임스페이스 트리의 각 노드(절대파일or절대dir경로)
에는 읽기-쓰기 잠금이 있다.
각마스터 작업은 실행 전에 읽기-쓰기 락을 얻는다.
예를들어 /d1/d2/.../dn/leaf관련된건 /d1뿐만아니라 leaf까지 읽기 잠금을 얻을수있다. / 풀 경로는 write
leaf는 작업에 따라 파일이나 디렉토리일 수 있다.

예를들어 /home/usr 가 /svae/usr에 스냅샷을 생성하는 동안 /home/usr/foo가 생성되지 않도록 잠금메커니즘
을 설명해보겟다
스냅샷 작업은 /home 및 /save의 읽기 잠금 및 /home/user 및 /save/user에 대한 쓰기 잠금을 가져옵니다
파일을 생성하면 /home 및 /home/user의 읽기 잠금과, 쓰기 잠금 장치 /home/user/foo를 사용할 수 있습니다. 
두 작업은 /home/user에서 충돌하는 잠금을 얻으려 하기 때문에 올바르게 직렬화됩니다. "디렉토리" 또는 
inode와 같은 데이터 구조가 수정으로부터 보호되지 않으므로 파일을 생성할 때 상위 디렉토리에 대한 쓰기
 잠금이 필요하지 않습니다. 

네임스페이스의 읽기 잠금은 상위디렉토리가 삭제되지않도록 보호하기 충분함. 이 잠금체게의 한가지 좋은
속성은 동일한 디렉토리에 동시 변이를 허용한다는거다. 예를들어 여러개의 파일생성을 동일한 디렉토리에서
동시에 실행할수있다. 
각 디렉토리는 디렉토리 이름에대한 읽기잠금과 파일이름 쓰기 잠금을 갖는다.
디렉토리의 읽기잠금은 디렉토리가 삭제, 이름변경, 스냅샷작성안되도록 충분.
파일이름에대한쓰기잠금은 같은이름의 파일을 두번 만드려는 시도를 직렬화한다. - 동시에해서같은파일만들어지면안대잖아

네임 스페이스는 많은 노드를 가질 수 있으므로 읽기 - 쓰기 잠금 객체는 느리게 할당되고 사용되지 않으면 
삭제됩니다. 또한 잠금은 교착 상태를 방지하기 위해 일관된 전체 순서로 획득됩니다. 즉, 네임 스페이스 
트리에서 수준순으로 정렬되고 같은 수준에서 사전 식으로 정렬됩니다.

4.2 복제본 배치하는 방법
복제는 데이터 신뢰성과 가용성을 극대화 한다.
일단 복제본은 같은 랙에 두면 안된다 다른랙에 둬야 손상되거나 오프라인상태도 청크 일부 복제본이 유지되고
사용가능한상태가능(리소스장애, 정전) 그럼 신뢰성
가용성은 대역폭 랙이 나눠져있으니까 트래픽을 나눌수있다 그러나 쓰기는 반대로 더많은 작업해야함 그건 어쩔수없음.

4.3 생성, 반복, 조정
마스터는 처음에 복제본을 배치할 위치 선택.
1. 디스크공간활용률이 평균 미만
2. 각 청크에서 최신파일 수를 제한. 왜냐면최신파일은 엄청난쓰기트래픽생길확률높고 나중엔 리드전용만된다.
3. 랙에 데이터 분산시키자고함. 마스터는 사용가능한 복제수가 오류로인해 일정아래면 다시 복제. 
다시 복제할때 우선순위는 거리도 고려사항, 더 많은 복제본이 손상된 부분이 우선, 최근 삭제된 파일에 속한 청크보다는 라이브파일에대한 청크가 우선
실행중인 응용파일 실패영향을 최소하 하기위해 클라이언트 진행을 차단하는 모든 청크 우선순위 높임

4.4 가비지 컬렉션
파일 삭제하면 저장소 바로삭제하지 않고 천천히함. 이게 훨씬 낫대.

4.4.1 메커니즘
파일 삭제하면 마스터가 다른 변경사항처럼 바로 즉시삭제 기록. 그러나 리소스는 즉시회수하지않고 파일삭제스탬프가 포함된 숨겨진이름으로
바뀜. 마스터는 파일시스템 네임스페이스를 정기적으로 스캔하는 동안 숨겨진파일이 3일이상(사용자지정가능)이면 그때 제거한다.
그때까지 새로운 특수이름으로 파일을 읽을수있고 파일을 정상으로 삭제할수있다. 숨겨진파일 네임스페이스에서 제거하면
해당 메모리와 메타데이터 지워짐. 이건 모든 청크에 대한 링크 모두삭제
청크 스페이스에서 어떤파일로부터로도 도달할수없는 청크 스캔해서 그 청크 메타데이터 삭제. 하트비트로 정기적으로 하위집합보고
마스터는 마스터에서 더이상존재하재 않는 ID로 응답 그러면 청크서버는 그걸 삭제

4.4.2 토론
바로 삭제하는거보다 장점
첫째	구성요소가 자주 발생하는 대규모시스템에서 신뢰할수있음 - 청크생성시 하다가 실패하면 마스터가 알지 못하는 복제본 남음
복제본삭제 메세지가 손실될수있으며 시스템에 다 알려야한다. 그런방법에서 더 좋음
둘째	백그라운드에 실행된다 가비지컬랙션은(하트비트, 스캔).  그래서 일괄적으로 이뤄지며 필요한리소스가 분할된다. 또한마스터가
쉴때 수행된다. 마스터는 적시에 요청하는 클라이언트에게 신속하게 응답한다.
셋째 	실수로한 삭제에 대한 안전망 제공. 단점은 자주삭제쓰는앱은 즉시 스토리지를 즉시재사용불가해서. 삭제된파일을 다시 명시적으로
삭제하면 스토리지 회수속도를 높여 해결. 이것도 사용자가 바로삭제될수잇도록 구현 가능

4.5 오래된복지제본감지
청크 버전 번호 부여한다. 이건 업데이트 할때마다 버전 업데이트. 이건 청크에 통지되기전에 발생. 발생해야지 청크에 쓰기시작
불가능하면 버전 안됨. 청크가 다시작되면 청크집합과 버전번호를 보고할때 마스터에서 오래된 청크 있는지 확인. 
마스터가 버전보다 큰 버전 보는 경우 이걸 최신버전으로 간주.
마스터는정기적으로 오래된 복제본 제거. 그전에는 클라이언트는 부실본 없다고 보고. 
다른 보호수단은 버전정보를 보고해서 항상 확인해서 최신버전을 유지.



5. 고장 및 진단
구성요소 고장은 아주 중요하다, 기계와 디스크를 완전히 신뢰 불가. 이경우를위해 진단하기 위해 구축한 도구설명

5.1 고가용성
클러스터는 수백개의 서버중 일부는 특정지점에서 사용불가. 이걸. 빠른 복구와 복제로 해결한당/
5.1.1 빠른복구
시스템은 종료상태와 상관없이 상태 복원을 초단위로 하게 설계.
5.1.2 청크복제
어느랙에서 손상된청크를 다른랙에서 다시 복제
5.1.3 마스터 복제
마스터는 신뢰성을 위해 복제. 로그, 검사점등 핵심 기능이 다 잇어서 여러 시스템에 복제 해둠. 마스터 망하면
바로 빽업에서 다시 마스터프로세스 시작 그리고 백업마스터는 마스터가 중단되도 파일읽기권한 제공
마스터가 권한주는것보다 이건 약간 지연되는데아주약간. 따라서 읽기가용성 향상. 
이런 백업(섀도우)마스터는 정보제강하기 위해 로그 복제본도 계속읽고 변경되는것도 계속 구조에 적용. 기존마스터와마찬가지
복제본 찾고 하느비트 해서 상태 모니터링 복제본생성삭제는 마스터의 권한 얜안됨.

5.2 데이터 무결성
데이터 손상시 청크간 복제본 비교하는건 비현실적. 더욱이 현재작업중인건 불가능.그래서 각청크는 체크섬으로 복사본의 무결성 독립적으로
확인해야함. 청크 64kb블록으로 분리하고 각각은 32비트 체크섬이있음, 메타데이터와 마찬가지로 체크섬 메모리에보관되고 로그화함께 영구저장
데이터 리딩이오면 값반환전에 모든 레플리카가 동일한 체크섬인지 확인. 다르다면 오류반환하고 마스터에게 불일치 보고.
이후 요청자는 다른 청크에서 읽고 마스터는 청크 복제 시작. 새복제본 되면 불일치한 복제본 삭제하도록 지시까지.
체크섬으로 오버헤드생길꺼라생각하는데 거의없음 체크섬블록경계에에서 읽기 정렬화하여 오버헤드줄이고 적은량만 추가하기때문에.
작업도 I/O작업없어서 동반으로 수행도가능해.
체그썸 계산은 어팬드에 크게 최적화됨. 왜냐면 마지막부분만 계속 계산하면되고. 손상부분은 머나중에는 또결국체크섬달라져서 손상확인가능.
청크 경계선은 마지막 블록 확인한 뒤 다음 쓰기를 수행해야한다. 안그러면 손상 숨길수도 있어서.
쉬는동안 청크는 비활성 청크 검색 체크. 그럼 손상된거체크하면 삭제하고 더 정확하게 가능. 그럼마스터가 더정확히 사용함.

5.3 진단 도구
로깅은 최소한의 일로 문제점을 해결, 디버깅, 성능 분석할수있다. 공간이 허용하는한 로그 최대한 유지.
문제점 대조도가능, 성능분석도 가능. 

6 측정
구현에서 병목이나 수치를 설명하기위헤 작은클러스털 벤치마크해봄.

6.1 마이크로벤치마크
1개 마스터 2개백업마스터 16청크서버 16클라이언트로 구성한테스트 - 미니멀한거임
사양 1.4g 2기가메모리, 5400알피엠 80기가 ~~~사양설명
6.1.1 리딩할때
N개 클라이언트 동시에 읽는다. 실제론 물리적한계로 80%정도 이고 많아지면 동시에읽을수있으니 좀더 75%까지떨굼
6.1.2 쓸때
여러레플리카에 한번에다써야되니까 좀 느리다. 클라이언트늘수록 같은 충돌이러날수잇으므로 느리다.이론의 절반정도
하지만 총대역폭엔 영향 미치지 않아서 큰 문제 되지 않음.
6.1.2 어팬드
같은ㅇ 파일에 추가할때  마지막파일청크ㅡㄹ저장하는 네트쿼크대역ㅍ폭에의해제한 그래서 클라이언트많을수럭
점점 속도떨어짐. 이건클라이언트상황마다 달라짐. 결국클라이언트네트쿼크가중요한다.

6.2 현실에서 클러스터
대표적으로 2개 클러스터가잇다
1. 연구목적으로 데이터 변환 분석 결과 다시쓰는 것
2. 실제로 오랜작업하는 것

6.2.1 스토리지
2처럼 파일 이큰경우는 실제로 많은 불필요한 파일이 잇을수 있다.

6.2.2 메타데이터
메타데이터는 매우 작으므로 복구가 엄청 빠르다. 거의 몇초밖에 안된다. 그러나 데이터 가져오는건 좀 오래걸린다.

6.2.3. 읽기 및 쓰기속도
읽기가 쓰기보다 훨씬 빠르다.

6.2.4 마스터로드
대부분의 시간은 대형 디렉토리를 찾는게 오래걸림. 이걸 효율적으로검색하기위해 마스터데이터구조를 이진검색으로 변경 
네임스페이스 앞에 이름름을 캐싱해서 속도향상

6.2.5. 복구시간
레플리카다를때 복제 ㅅ시간은 리소스양에 따다름
테스트결과 한 청크 서버 660G 죽엿을때 23분정도록빠르게복원
두개 터쳤을땐 더높은우선순위가되서 2분이내 복구 완료

6.3 작업공간고장
x-연구개발/y-실제상황
6.3.1 방버버 및 주의사항
너무 일반화시키면 안된다 GFS와 해당 앱은 서로 맞게 조정되는 경향이 있엇고 앱도 GFS에 맞춰 설계됫다.
6.3.2청크 서버작업량
대개 모든종류작업은 256kb를 넘는 작업을 한다. 작은 크기는 중요한 핵심데이터다.대게..?
6.3.3 어팬드대 라이트 비교
추가가 큰 경우가 많다. 표 참고 데이터 롸이트는 0,0001정도고 어팬드는 0,0003 미만
대부분 덮어쓰기는 시간 초과또는 오류로 클라이언트 재시작?
6.3.4 마스터 작업부하
연구용보다 실제사용이 훨신 많다.

7. 경험
GFS가 백엔드로 되잇다. 시간지나서 연구 개발작헙 포함하도록 진화.
가끔 리눅스와 드라이버 프로토콜이 때로는 불일치 그래서 커널 수정
기존은 락걸걸어서 같은 데이터 접근할때 병목생겻는데 그러면 메모리보단 네트워크 문제이므로
mmap 대신 pread함수로 바꿈 여분의 복사본으로 이건 커뮤니티에서 적절한커널개선하는중.

8. 비슷한 것들과비교
일관성을 위해 중앙집중식접근법 선택. 이건 마스터가 왕이므로 훨씬 쉽게 구현
NAsd랑 큰차이점은 청크크기가 가변/고정
raid랑은 접근법이 복제가 간단.
AFS, xFS, Frangipani [12] 및 Intermezzo[6]와 같은 시스템과 달리 이건 대규모 네트워크 세트를 다루기때문에
재사용률이 낮으므로 인터페이스아래 캐싱제공안함.

9, 결론
GFS는 대규모 데이터 처리작업에 필수적인 자질을 갖게됨. 대부분의 사람들의 작업에 적용 가능.
현지와 다른 데이터크기에 과거 파일시스템 문제를 고려함. 그결과 오류를 예외가 아닌 표준으로 처리해야햇고
거대한 파일을 수정을 최적화하고 파일시스템을 완화. 인터페이스 사용. 지속적인 모니터링 자동복구가 있어야함.
또한 피해를 복구하는 걸 빨리 보완하는 매커니즘 만들고 체크섬으로 데이터 손상 감지. 이건 디스크 많음 예외가아니라일반
로그도남기고 파일제어를 마스터와 청크로 분리해서 클라이언트가 따로접근함. 공동작업은 청크임대로 해결
이렇게하면 중앙집중가능. 네트워크가 향상되면 개별클라이언트가쓰는거 제한 완화
스토리지 요구사항 충족 연구 , 처리로위한 구글에서 사용. 중요하고 혁신적인 도구가됨.

